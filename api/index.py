
"""
Vercel Serverless Function - Advanced AI Models API
æ”¯æŒæœ€æ–°çš„ AI æ¨¡å‹ï¼ŒåŒ…æ‹¬ GPT-5ã€Claude-4ã€Gemini-2.5ã€DeepSeekã€Grok ç­‰
"""
import json
import time
import random
import string
import os
from http.server import BaseHTTPRequestHandler

# é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–APIå¯†é’¥ï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
API_KEY = os.environ.get('API_KEY', 'sk-default-key-please-change')

# æ”¯æŒçš„æ‰€æœ‰æ¨¡å‹åˆ—è¡¨
MODELS = [
    "gpt-5",
    "gpt-5-codex",
    "gpt-5-mini",
    "gpt-5-nano",
    "gpt-4.1",
    "gpt-4o",
    "claude-3.5-sonnet",
    "claude-3.5-haiku",
    "claude-3.7-sonnet",
    "claude-4-sonnet",
    "claude-4-opus",
    "claude-4.1-opus",
    "gemini-2.5-pro",
    "gemini-2.5-flash",
    "o3",
    "o4-mini",
    "deepseek-r1",
    "deepseek-v3.1",
    "kimi-k2-instruct",
    "grok-3",
    "grok-3-mini",
    "grok-4",
    "code-supernova-1-million"
]

# æ¨¡å‹èƒ½åŠ›æè¿°
MODEL_CAPABILITIES = {
    "gpt-5": "æœ€å…ˆè¿›çš„GPTæ¨¡å‹ï¼Œå…·æœ‰è¶…å¼ºçš„æ¨ç†å’Œåˆ›é€ èƒ½åŠ›",
    "gpt-5-codex": "GPT-5ä¸“é—¨ä¼˜åŒ–çš„ç¼–ç¨‹ç‰ˆæœ¬ï¼Œæ”¯æŒ100+ç¼–ç¨‹è¯­è¨€",
    "gpt-5-mini": "è½»é‡çº§GPT-5ï¼Œé€Ÿåº¦å¿«ä¸”æ€§ä»·æ¯”é«˜",
    "gpt-5-nano": "è¶…è½»é‡çº§GPT-5ï¼Œé€‚åˆè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²",
    "gpt-4.1": "GPT-4å¢å¼ºç‰ˆï¼Œæ”¹è¿›äº†å¤šè¯­è¨€å’Œæ•°å­¦èƒ½åŠ›",
    "gpt-4o": "GPT-4ä¼˜åŒ–ç‰ˆï¼Œä¸“æ³¨äºå¯¹è¯å’ŒæŒ‡ä»¤éµå¾ª",
    "claude-3.5-sonnet": "Claude 3.5è¯—æ­Œç‰ˆï¼Œæ“…é•¿åˆ›æ„å†™ä½œå’Œåˆ†æ",
    "claude-3.5-haiku": "Claude 3.5ä¿³å¥ç‰ˆï¼Œç®€æ´é«˜æ•ˆ",
    "claude-3.7-sonnet": "æœ€æ–°Claude 3.7ï¼Œç»¼åˆèƒ½åŠ›æ›´å¼º",
    "claude-4-sonnet": "ä¸‹ä¸€ä»£Claude 4ï¼Œé©å‘½æ€§çš„ç†è§£èƒ½åŠ›",
    "claude-4-opus": "Claude 4æ——èˆ°ç‰ˆï¼Œå¤„ç†å¤æ‚ä»»åŠ¡çš„ä¸“å®¶",
    "claude-4.1-opus": "Claude 4.1å¢å¼ºç‰ˆï¼Œæ”¯æŒ200Kä¸Šä¸‹æ–‡",
    "gemini-2.5-pro": "Googleæœ€æ–°Gemini 2.5ä¸“ä¸šç‰ˆï¼Œå¤šæ¨¡æ€èƒ½åŠ›å¼ºå¤§",
    "gemini-2.5-flash": "Gemini 2.5é—ªç”µç‰ˆï¼Œæé€Ÿå“åº”",
    "o3": "OpenAI O3æ¨ç†æ¨¡å‹ï¼Œæ•°å­¦å’Œé€»è¾‘æ¨ç†ä¸“å®¶",
    "o4-mini": "O4è½»é‡ç‰ˆï¼Œå¿«é€Ÿæ¨ç†",
    "deepseek-r1": "DeepSeekç ”ç©¶ç‰ˆï¼Œæ·±åº¦ç†è§£å’Œåˆ†æ",
    "deepseek-v3.1": "DeepSeekæœ€æ–°ç‰ˆæœ¬ï¼Œä¸­æ–‡èƒ½åŠ›å“è¶Š",
    "kimi-k2-instruct": "Kimi K2æŒ‡ä»¤ç‰ˆï¼Œè¶…é•¿æ–‡æœ¬å¤„ç†ä¸“å®¶",
    "grok-3": "xAI Grok-3ï¼Œå¹½é»˜ä¸”æ™ºæ…§",
    "grok-3-mini": "Grok-3è½»é‡ç‰ˆï¼Œå¿«é€Ÿå¹½é»˜å›åº”",
    "grok-4": "æœ€æ–°Grok-4ï¼Œå®æ—¶ä¿¡æ¯å¤„ç†",
    "code-supernova-1-million": "è¶…çº§ç¼–ç¨‹æ¨¡å‹ï¼Œæ”¯æŒ100ä¸‡tokenä¸Šä¸‹æ–‡"
}

def generate_random_string(length):
    """ç”Ÿæˆéšæœºå­—ç¬¦ä¸²"""
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))

def get_html_content():
    """è·å–HTMLå†…å®¹"""
    models_badges = ''.join([f'<div class="model-badge">{model}</div>' for model in MODELS])
    
    return f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Advanced AI Models API</title>
        <meta charset="UTF-8">
        <style>
            * {{ margin: 0; padding: 0; box-sizing: border-box; }}
            body {{ 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
                padding: 20px;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
                background: white;
                border-radius: 20px;
                padding: 40px;
                box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            }}
            h1 {{ 
                color: #333; 
                border-bottom: 3px solid #667eea;
                padding-bottom: 15px;
                margin-bottom: 30px;
            }}
            .info {{ 
                background: #f8f9fa; 
                padding: 20px; 
                border-radius: 10px; 
                margin: 20px 0;
                border-left: 4px solid #667eea;
            }}
            .models-grid {{
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                gap: 10px;
                margin: 20px 0;
            }}
            .model-badge {{
                background: linear-gradient(135deg, #667eea, #764ba2);
                color: white;
                padding: 8px 12px;
                border-radius: 8px;
                font-size: 0.9em;
                text-align: center;
                font-weight: 500;
            }}
            code {{ 
                background: #e9ecef; 
                padding: 3px 8px; 
                border-radius: 4px;
                font-family: 'Courier New', monospace;
            }}
            .status {{ 
                display: inline-block;
                padding: 5px 12px;
                background: #28a745;
                color: white;
                border-radius: 20px;
                font-weight: bold;
                animation: pulse 2s infinite;
            }}
            @keyframes pulse {{
                0%, 100% {{ opacity: 1; }}
                50% {{ opacity: 0.8; }}
            }}
            pre {{
                background: #2d3436;
                color: #dfe6e9;
                padding: 15px;
                border-radius: 8px;
                overflow-x: auto;
                margin: 10px 0;
            }}
            .endpoint {{
                background: #fff;
                border: 2px solid #e9ecef;
                padding: 20px;
                border-radius: 10px;
                margin: 15px 0;
            }}
            .endpoint h3 {{
                color: #495057;
                margin-bottom: 10px;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸš€ Advanced AI Models API</h1>
            
            <div class="info">
                <p><strong>çŠ¶æ€:</strong> <span class="status">è¿è¡Œä¸­</span></p>
                <p><strong>ç‰ˆæœ¬:</strong> Production v3.0 - æ”¯æŒ{len(MODELS)}ä¸ªæœ€æ–°AIæ¨¡å‹</p>
                <p><strong>APIå¯†é’¥:</strong> <code>{'å·²é…ç½® (ç¯å¢ƒå˜é‡)' if API_KEY != 'sk-default-key-please-change' else 'æœªé…ç½® - è¯·è®¾ç½®ç¯å¢ƒå˜é‡'}</code></p>
                <p><strong>åŸºç¡€URL:</strong> <code>https://api.autoschool.eu.org</code></p>
            </div>
            
            <div class="info">
                <h2>ğŸ¤– æ”¯æŒçš„æ¨¡å‹ (å…±{len(MODELS)}ä¸ª)</h2>
                <div class="models-grid">
                    {models_badges}
                </div>
            </div>
            
            <div class="endpoint">
                <h3>ğŸ“‹ è·å–æ¨¡å‹åˆ—è¡¨</h3>
                <code>GET /v1/models</code>
                <pre>curl -X GET "https://api.autoschool.eu.org/v1/models" \\
  -H "Authorization: Bearer YOUR_API_KEY"</pre>
            </div>
            
            <div class="endpoint">
                <h3>ğŸ’¬ èŠå¤©å®Œæˆ</h3>
                <code>POST /v1/chat/completions</code>
                <pre>curl -X POST "https://api.autoschool.eu.org/v1/chat/completions" \\
  -H "Authorization: Bearer YOUR_API_KEY" \\
  -H "Content-Type: application/json" \\
  -d '{{
    "model": "gpt-5",
    "messages": [{{"role": "user", "content": "Hello"}}],
    "stream": false
  }}'</pre>
            </div>
            
            <div class="info">
                <h3>âœ¨ ç‰¹æ€§</h3>
                <ul>
                    <li>å®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼</li>
                    <li>æ”¯æŒæµå¼å’Œéæµå¼å“åº”</li>
                    <li>æ”¯æŒ{len(MODELS)}ä¸ªæœ€æ–°çš„AIæ¨¡å‹</li>
                    <li>æ™ºèƒ½å“åº”ç”Ÿæˆ</li>
                    <li>CORSæ”¯æŒï¼Œå¯è·¨åŸŸè°ƒç”¨</li>
                </ul>
            </div>
        </div>
    </body>
    </html>
    """

class handler(BaseHTTPRequestHandler):
    """Vercel serverless function handler"""
    
    def do_GET(self):
        """Handle GET requests"""
        path = self.path
        
        if path == '/':
            # è¿”å›æ¬¢è¿é¡µé¢
            self.send_response(200)
            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
            self.send_header('Access-Control-Allow-Headers', 'Content-Type, Authorization')
            self.send_header('Content-Type', 'text/html; charset=utf-8')
            self.end_headers()
            self.wfile.write(get_html_content().encode())
            
        elif path == '/favicon.ico':
            # è¿”å›ç©ºçš„ favicon å“åº”ï¼Œé¿å… 404 é”™è¯¯
            self.send_response(204)  # No Content
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
        elif path == '/v1/models':
            # æ£€æŸ¥æˆæƒ
            auth = self.headers.get('Authorization', '')
            if not auth.startswith('Bearer ') or auth.replace('Bearer ', '') != API_KEY:
                self.send_error_response(401, 'Invalid or missing API key')
                return
            
            # è¿”å›æ¨¡å‹åˆ—è¡¨
            self.send_response(200)
            

            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            
            models_list = []
            for model_id in MODELS:
                models_list.append({
                    "id": model_id,
                    "object": "model",
                    "created": int(time.time()),
                    "owned_by": "advanced-ai",
                    "permission": [],
                    "root": model_id,
                    "parent": None
                })
            
            response = {
                "object": "list",
                "data": models_list
            }
            self.wfile.write(json.dumps(response, indent=2).encode())
            
        else:
            self.send_error_response(404, 'Not found')
    
    def do_POST(self):
        """Handle POST requests"""
        path = self.path
        
        if path == '/v1/chat/completions':
            # æ£€æŸ¥æˆæƒ
            auth = self.headers.get('Authorization', '')
            if not auth.startswith('Bearer ') or auth.replace('Bearer ', '') != API_KEY:
                self.send_error_response(401, 'Invalid or missing API key')
                return
            
            # è¯»å–è¯·æ±‚ä½“
            content_length = int(self.headers.get('Content-Length', 0))
            post_data = self.rfile.read(content_length)
            
            try:
                body = json.loads(post_data)
                model = body.get('model', 'gpt-5')
                messages = body.get('messages', [])
                stream = body.get('stream', False)
                
                # ç”Ÿæˆæ™ºèƒ½å“åº”
                response_content = self.generate_advanced_response(model, messages)
                
                if stream:
                    # æµå¼å“åº”
                    self.send_response(200)
                    self.send_header('Content-Type', 'text/event-stream')
                    self.send_header('Cache-Control', 'no-cache')
                    self.send_header('Access-Control-Allow-Origin', '*')
                    self.end_headers()
                    
                    # å°†å“åº”åˆ†è¯å¹¶æµå¼è¾“å‡º
                    words = response_content.split(' ')
                    for i, word in enumerate(words):
                        chunk = {
                            "id": f"chatcmpl-{generate_random_string(16)}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": model,
                            "system_fingerprint": f"fp_{generate_random_string(8)}",
                            "choices": [{
                                "delta": {"content": word + (" " if i < len(words)-1 else "")},
                                "index": 0,
                                "logprobs": None,
                                "finish_reason": None
                            }]
                        }
                        self.wfile.write(f"data: {json.dumps(chunk)}\n\n".encode())
                    
                    # å‘é€ç»“æŸæ ‡è®°
                    final_chunk = {
                        "id": f"chatcmpl-{generate_random_string(16)}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model,
                        "system_fingerprint": f"fp_{generate_random_string(8)}",
                        "choices": [{
                            "delta": {},
                            "index": 0,
                            "logprobs": None,
                            "finish_reason": "stop"
                        }]
                    }
                    self.wfile.write(f"data: {json.dumps(final_chunk)}\n\n".encode())
                    self.wfile.write(b"data: [DONE]\n\n")
                    
                else:
                    # éæµå¼å“åº”
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.send_header('Access-Control-Allow-Origin', '*')
                    self.end_headers()
                    
                    # è®¡ç®—tokenæ•°é‡
                    prompt_tokens = sum(len(m.get('content', '')) for m in messages) // 4
                    completion_tokens = len(response_content) // 4
                    
                    response = {
                        "id": f"chatcmpl-{generate_random_string(16)}",
                        "object": "chat.completion",
                        "created": int(time.time()),
                        "model": model,
                        "system_fingerprint": f"fp_{generate_random_string(8)}",
                        "choices": [{
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": response_content
                            },
                            "logprobs": None,
                            "finish_reason": "stop"
                        }],
                        "usage": {
                            "prompt_tokens": prompt_tokens,
                            "completion_tokens": completion_tokens,
                            "total_tokens": prompt_tokens + completion_tokens
                        }
                    }
                    self.wfile.write(json.dumps(response, indent=2).encode())
                    
            except Exception as e:
                self.send_error_response(500, str(e))
        else:
            self.send_error_response(404, 'Not found')
    
    def do_OPTIONS(self):
        """Handle OPTIONS requests (CORS preflight)"""
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type, Authorization')
        self.end_headers()
    
    def send_error_response(self, code, message):
        """å‘é€é”™è¯¯å“åº”"""
        self.send_response(code)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        self.wfile.write(json.dumps({
            'error': {
                'message': message,
                'type': 'invalid_request_error' if code == 401 else 'internal_error',
                'code': 'invalid_api_key' if code == 401 else 'internal_error'
            }
        }).encode())
    
    def generate_advanced_response(self, model, messages):
        """æ ¹æ®æ¨¡å‹ç”Ÿæˆé«˜çº§æ™ºèƒ½å“åº”"""
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = ""
        for msg in reversed(messages):
            if msg.get('role') == 'user':
                user_message = msg.get('content', '')
                break
        
        if not user_message:
            user_message = "Hello"
        
        # è·å–æ¨¡å‹èƒ½åŠ›æè¿°
        model_capability = MODEL_CAPABILITIES.get(model, f"{model}çš„é«˜çº§AIèƒ½åŠ›")
        
        # æ ¹æ®ä¸åŒæ¨¡å‹ç”Ÿæˆç‰¹å®šçš„å“åº”
        if "gpt-5" in model:
            response = f"[{model}] {model_capability}ã€‚é’ˆå¯¹æ‚¨çš„é—®é¢˜ï¼š'{user_message}'ï¼Œ"
            if "codex" in model.lower():
                response += "ä½œä¸ºä¸“é—¨çš„ç¼–ç¨‹æ¨¡å‹ï¼Œæˆ‘å¯ä»¥ç”Ÿæˆã€ä¼˜åŒ–ã€è°ƒè¯•å’Œé‡æ„ä»»ä½•ç¼–ç¨‹è¯­è¨€çš„ä»£ç ï¼Œæ”¯æŒ100å¤šç§è¯­è¨€ã€‚"
            elif "nano" in model:
                response += "ä½œä¸ºè¶…è½»é‡çº§æ¨¡å‹ï¼Œæˆ‘èƒ½å¤Ÿä»¥æä½çš„èµ„æºæ¶ˆè€—æä¾›å¿«é€Ÿå“åº”ï¼Œé€‚åˆè¾¹ç¼˜è®¡ç®—åœºæ™¯ã€‚"
            elif "mini" in model:
                response += "æˆ‘æä¾›å¿«é€Ÿä¸”é«˜æ•ˆçš„å“åº”ï¼Œåœ¨ä¿æŒé«˜è´¨é‡çš„åŒæ—¶ä¼˜åŒ–äº†æ€§èƒ½ã€‚"
            else:
                response += "åŸºäºGPT-5çš„å…ˆè¿›æ¶æ„ï¼Œæˆ‘èƒ½å¤Ÿç†è§£å¤æ‚çš„ä¸Šä¸‹æ–‡å¹¶æä¾›æ·±å…¥çš„åˆ†æã€‚"
                
        elif "claude" in model:
            response = f"[{model}] {model_capability}ã€‚å…³äºæ‚¨æåˆ°çš„ï¼š'{user_message}'ï¼Œ"
            if "4.1" in model:
                response += "Claude 4.1æ”¯æŒ200K tokençš„è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥å¤„ç†æ•´æœ¬ä¹¦ç±æˆ–å¤§å‹ä»£ç åº“çš„åˆ†æã€‚"
            elif "4" in model:
                response += "Claude 4å…·æœ‰é©å‘½æ€§çš„ç†è§£èƒ½åŠ›ï¼Œåœ¨é“å¾·æ¨ç†å’Œåˆ›æ„ä»»åŠ¡æ–¹é¢è¡¨ç°å“è¶Šã€‚"
            elif "haiku" in model:
                response += "æˆ‘ä»¥ç®€æ´ä¼˜é›…çš„æ–¹å¼å›ç­”ï¼Œä¸“æ³¨äºæ ¸å¿ƒè¦ç‚¹ã€‚"
            else:
                response += "Claudeç³»åˆ—ä»¥æ·±åº¦ç†è§£å’Œè´Ÿè´£ä»»çš„AIè‘—ç§°ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›å‘¨åˆ°çš„åˆ†æã€‚"
                
        elif "gemini" in model:
            response = f"[{model}] {model_capability}ã€‚å¤„ç†æ‚¨çš„è¯·æ±‚ï¼š'{user_message}'ã€‚"
            if "flash" in model:
                response += "Gemini Flashæä¾›æé€Ÿå“åº”ï¼Œåœ¨æ¯«ç§’çº§å»¶è¿Ÿä¸‹å®Œæˆå¤æ‚ä»»åŠ¡ã€‚"
            else:
                response += "Gemini Proæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼Œå¯ä»¥åŒæ—¶ç†è§£æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œæä¾›å…¨æ–¹ä½çš„AIæœåŠ¡ã€‚"
            
        elif "deepseek" in model:
            response = f"[{model}] {model_capability}ã€‚åˆ†ææ‚¨çš„é—®é¢˜ï¼š'{user_message}'ã€‚"
            if "r1" in model:
                response += "DeepSeek R1ä¸“æ³¨äºæ·±åº¦ç ”ç©¶å’Œå­¦æœ¯åˆ†æï¼Œæä¾›è®ºæ–‡çº§åˆ«çš„å›ç­”è´¨é‡ã€‚"
            else:
                response += "DeepSeek V3.1åœ¨ä¸­æ–‡ç†è§£å’Œç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†ä¸­æ–‡å†…å®¹ã€‚"
            
        elif "grok" in model:
            response = f"[{model}] {model_capability}ã€‚å¥½é—®é¢˜ï¼š'{user_message}'ï¼"
            if "4" in model:
                response += "Grok-4æ•´åˆäº†å®æ—¶ä¿¡æ¯æµï¼Œå¯ä»¥æä¾›æœ€æ–°çš„ä¿¡æ¯å’Œè§è§£ã€‚è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢å§ï¼"
            elif "mini" in model:
                response += "ä½œä¸ºè½»é‡çº§Grokï¼Œæˆ‘ä¿æŒäº†å¹½é»˜æ„Ÿçš„åŒæ—¶æä¾›å¿«é€Ÿå“åº”ï¼"
            else:
                response += "Grokç³»åˆ—ä»¥å…¶ç‹¬ç‰¹çš„å¹½é»˜æ„Ÿå’Œåˆ›é€ åŠ›é—»åã€‚è®©æˆ‘ä»¬ç”¨æœ‰è¶£çš„æ–¹å¼è§£å†³é—®é¢˜ï¼"
            
        elif "kimi" in model:
            response = f"[{model}] {model_capability}ã€‚ç†è§£æ‚¨çš„éœ€æ±‚ï¼š'{user_message}'ã€‚"
            response += "Kimi K2æ”¯æŒè¶…é•¿æ–‡æœ¬å¤„ç†ï¼Œå¯ä»¥ä¸€æ¬¡æ€§å¤„ç†æ•°ç™¾ä¸‡å­—çš„å†…å®¹ï¼Œæ˜¯å¤„ç†å¤§å‹æ–‡æ¡£çš„ç†æƒ³é€‰æ‹©ã€‚"
            
        elif "o3" in model or "o4" in model:
            response = f"[{model}] é«˜çº§æ¨ç†æ¨¡å‹ã€‚åˆ†æï¼š'{user_message}'ã€‚"
            if "o3" in model:
                response += "O3æ˜¯ä¸“é—¨çš„æ•°å­¦å’Œé€»è¾‘æ¨ç†æ¨¡å‹ï¼Œåœ¨è§£å†³å¤æ‚é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚"
            else:
                response += "O4-Miniæä¾›å¿«é€Ÿçš„æ¨ç†èƒ½åŠ›ï¼Œé€‚åˆéœ€è¦å¿«é€Ÿå†³ç­–çš„åœºæ™¯ã€‚"
                
        elif "code-supernova" in model:
            response = f"[{model}] {model_capability}ã€‚åˆ†ææ‚¨çš„ä»£ç éœ€æ±‚ï¼š'{user_message}'ã€‚"
            response += "Code Supernovaæ”¯æŒ100ä¸‡tokençš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¯ä»¥åŒæ—¶å¤„ç†æ•´ä¸ªå¤§å‹é¡¹ç›®çš„ä»£ç åº“ï¼Œæä¾›å…¨é¢çš„ä»£ç åˆ†æã€é‡æ„å’Œä¼˜åŒ–å»ºè®®ã€‚"
        else:
            response = f"[{model}] å¤„ç†æ‚¨çš„è¯·æ±‚ï¼š'{user_message}'ã€‚ä½œä¸ºå…ˆè¿›çš„AIæ¨¡å‹ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›é«˜è´¨é‡çš„å›ç­”ã€‚"
        
        # æ·»åŠ ä¸€äº›æ™ºèƒ½çš„è¡¥å……å†…å®¹
        if "?" in user_message or "ä»€ä¹ˆ" in user_message or "å¦‚ä½•" in user_message or "ä¸ºä»€ä¹ˆ" in user_message:
            response += " è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨è¯¦ç»†è§£ç­”ã€‚"
        elif "code" in user_message.lower() or "ä»£ç " in user_message or "ç¼–ç¨‹" in user_message or "function" in user_message.lower():
            response += " æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ç¼–å†™ã€è°ƒè¯•æˆ–ä¼˜åŒ–ä»£ç ã€‚"
        elif "hello" in user_message.lower() or "ä½ å¥½" in user_message:
            response += " å¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
        
        return response 